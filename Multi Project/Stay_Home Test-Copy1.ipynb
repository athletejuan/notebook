{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import calendar\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# # check core SDK version number\n",
    "# print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "myworkspace\tsoutheastasia\tshrg\tsoutheastasia\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'Stay_Home'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. cpucluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpucluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok = True)\n",
    "\n",
    "# urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'train-images.gz'))\n",
    "# urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'train-labels.gz'))\n",
    "# urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', filename=os.path.join(data_folder, 'test-images.gz'))\n",
    "# urllib.request.urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', filename=os.path.join(data_folder, 'test-labels.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure utils.py is in the same directory as this code\n",
    "from utils import load_data\n",
    "\n",
    "# Data import\n",
    "# df=pd.read_csv('subway_merge_1.csv', encoding = 'utf-8')\n",
    "data=pd.read_csv(\"Cloud&UV08-17 danger25.csv\", engine='python', header=0).dropna()\n",
    "# data2=pd.read_csv(\"highway.csv\", engine='python', header=0)\n",
    "# data3=pd.read_csv(\"THSeoulCrime.csv\", engine='python', header=0)\n",
    "\n",
    "x=data.iloc[:,:3]\n",
    "y=data.iloc[:,-1:].values.reshape(-1,1)\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# x2=data2.iloc[:,:1]\n",
    "# y2=data2.iloc[:,-1:].values.reshape(-1,1)\n",
    "# xtrain2,xtest2,ytrain2,ytest2=train_test_split(x2,y2,test_size=0.3,random_state=0)\n",
    "\n",
    "# x3=data3['Month'].values.reshape(-1,1)\n",
    "# y3=data3['Crime.occur'].values.reshape(-1,1)\n",
    "# xtrain3,xtest3,ytrain3,ytest3=train_test_split(x3,y3,test_size=0.3,random_state=0)\n",
    "\n",
    "# x1=data.values[:,:2]\n",
    "# y1=data.values[:,-1:]\n",
    "# plt.figure()\n",
    "# plt.ylabel(\"day\")\n",
    "# plt.xlabel(\"month\")\n",
    "# plt.scatter(data['월'],data['일'],c=data['자외선위험'],alpha=0.5)\n",
    "# plt.show()\n",
    "\n",
    "# x2_1=data2['time'].values.reshape(-1,1)\n",
    "# y2_1=data2['hacc.ratio'].values.reshape(-1,1)\n",
    "# xtrain2_1,xtest2_1,ytrain2_1,ytest2_1=train_test_split(x2_1,y2_1,test_size=0.3,random_state=0)\n",
    "# plt.scatter(xtrain2_1,ytrain2_1)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AzureBlob myworkspstorage66603882c azureml-blobstore-750332db-0d0f-4fd6-9d39-b0214037ef7c\n",
      "Uploading C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\test-images.gz\n",
      "Uploading C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\test-labels.gz\n",
      "Uploading C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\train-images.gz\n",
      "Uploading C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\train-labels.gz\n",
      "Uploaded C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\test-labels.gz, 1 files out of an estimated total of 4\n",
      "Uploaded C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\train-labels.gz, 2 files out of an estimated total of 4\n",
      "Uploaded C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\test-images.gz, 3 files out of an estimated total of 4\n",
      "Uploaded C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\data\\train-images.gz, 4 files out of an estimated total of 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_06ae8a84d57544f1a82466eab0631909"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "# X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "# X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "# y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "# y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "\n",
    "# # now let's show some randomly chosen images from the traininng set.\n",
    "# count = 0\n",
    "# sample_size = 30\n",
    "# plt.figure(figsize = (16, 6))\n",
    "# for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "#     count = count + 1\n",
    "#     plt.subplot(1, sample_size, count)\n",
    "#     plt.axhline('')\n",
    "#     plt.axvline('')\n",
    "#     plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "#     plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# ds = ws.get_default_datastore()\n",
    "# print(ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "# ds.upload(src_dir=data_folder, target_path='SH', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder  = os.path.join(os.getcwd(), \"Stay_Home_script\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting C:\\Users\\Administrator\\Documents\\GitHub\\First-Step\\Test\\Stay_Home_script/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import sys\n",
    "import calendar\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "from azureml.core import Run\n",
    "from utils import load_data\n",
    "\n",
    "# let user feed in 2 parameters, the location of the data files (from datastore), and the regularization rate of the logistic regression model\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "# parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# data_folder = args.data_folder\n",
    "# print('Data folder:', data_folder)\n",
    "data=pd.read_csv(\"Cloud&UV08-17 danger25.csv\", engine='python', header=0).dropna()\n",
    "# data2=pd.read_csv(\"highway.csv\", engine='python', header=0)\n",
    "# data3=pd.read_csv(\"THSeoulCrime.csv\", engine='python', header=0)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "x=data.iloc[:,:3]\n",
    "y=data.iloc[:,-1:].values.reshape(-1,1)\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.3,random_state=0)\n",
    "\n",
    "# x2=data2.iloc[:,:1]\n",
    "# y2=data2.iloc[:,-1:].values.reshape(-1,1)\n",
    "# xtrain2,xtest2,ytrain2,ytest2=train_test_split(x2,y2,test_size=0.3,random_state=0)\n",
    "\n",
    "# x3=data3['Month'].values.reshape(-1,1)\n",
    "# y3=data3['Crime.occur'].values.reshape(-1,1)\n",
    "# xtrain3,xtest3,ytrain3,ytest3=train_test_split(x3,y3,test_size=0.3,random_state=0)\n",
    "\n",
    "# y=int(input(\"년도:\"))\n",
    "# m=int(input(\"월:\"))\n",
    "# d=int(input(\"일:\"))\n",
    "# h=int(input(\"시간:\"))\n",
    "# today=datetime.date(y,m,d).isocalendar()[1:]\n",
    "# np.where(obj2[\"주차요일\"] == today)\n",
    "# select_indices = list(np.where(obj2[\"주차요일\"] == today)[0])\n",
    "# top20=obj2.iloc[select_indices]\n",
    "# SUBWAY_Congestion=top20.head(20)\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# print('Train a logistic regression model with regularization rate of', args.reg)\n",
    "# K=3\n",
    "# KNN=KNeighborsClassifier(K, weights='distance')\n",
    "# KNN.fit(xtrain,ytrain)\n",
    "LR = LogisticRegression()\n",
    "LR.fit(xtrain, ytrain)\n",
    "# KNN.fit(xtrain2,ytrain2)\n",
    "# KNN.fit(xtrain3,ytrain3)\n",
    "\n",
    "print('Predict the test set')\n",
    "# KResult=KNN.predict(xtest)\n",
    "LResult=LR.predict(xtest)\n",
    "# KResult2=KNN.predict(xtest2)\n",
    "# KResult3=KNN.predict(xtest3)\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "Accuracy=metrics.accuracy_score(ytest,LResult)\n",
    "# Accuracy2=metrics.accuracy_score(ytest2,KResult2)\n",
    "# Accuracy3=metrics.accuracy_score(ytest3,KResult3)\n",
    "print('Accuracy is', Accuracy)\n",
    "# print('Accuracy is', Accuracy2)\n",
    "# print('Accuracy is', Accuracy3)\n",
    "\n",
    "# predict\n",
    "# KResult_1=KNN.predict([[m,d,0]])\n",
    "# KResult2_2=KNN.predict([[h]])\n",
    "# KResult3_2=KNN.predict([[m]])\n",
    "\n",
    "# run.log('regularization rate', np.float(args.reg))\n",
    "run.log('accuracy', np.float(Accuracy))\n",
    "\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=LR, filename='outputs/Stay_Home_model.pkl')\n",
    "\n",
    "# # note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "# X_train = load_data(os.path.join(data_folder, 'train-images.gz'), False) / 255.0\n",
    "# X_test = load_data(os.path.join(data_folder, 'test-images.gz'), False) / 255.0\n",
    "# y_train = load_data(os.path.join(data_folder, 'train-labels.gz'), True).reshape(-1)\n",
    "# y_test = load_data(os.path.join(data_folder, 'test-labels.gz'), True).reshape(-1)\n",
    "# print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# # get hold of the current run\n",
    "# run = Run.get_context()\n",
    "\n",
    "# print('Train a logistic regression model with regularization rate of', args.reg)\n",
    "# clf = LogisticRegression(C=1.0/args.reg, random_state=42)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# print('Predict the test set')\n",
    "# y_hat = clf.predict(X_test)\n",
    "\n",
    "# # calculate accuracy on the prediction\n",
    "# acc = np.average(y_hat == y_test)\n",
    "# print('Accuracy is', acc)\n",
    "\n",
    "# run.log('regularization rate', np.float(args.reg))\n",
    "# run.log('accuracy', np.float(acc))\n",
    "\n",
    "# os.makedirs('outputs', exist_ok=True)\n",
    "# # note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "# joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Documents\\\\GitHub\\\\First-Step\\\\Test\\\\Stay_Home_script\\\\utils.py'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.copy('utils.py', script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "# script_params = {\n",
    "#     '--data-folder': ds.path('SH').as_mount(),\n",
    "#     '--regularization': 0.8\n",
    "# }\n",
    "\n",
    "est = Estimator(source_directory=script_folder,\n",
    "#                 script_params=script_params,\n",
    "                compute_target=compute_target,\n",
    "                entry_script='train.py',\n",
    "                conda_packages=['scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>Stay_Home</td><td>Stay_Home_1558486519_b85fce0a</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/5754d318-4f03-4b3c-8aa4-e0ae873d15f8/resourceGroups/shrg/providers/Microsoft.MachineLearningServices/workspaces/myworkspace/experiments/Stay_Home/runs/Stay_Home_1558486519_b85fce0a\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: Stay_Home,\n",
       "Id: Stay_Home_1558486519_b85fce0a,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Queued)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d26beda2004d42a62e88d300219a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ActivityFailedException",
     "evalue": "Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"No module named 'pandas'\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"ModuleNotFoundError\",\n            \"message\": \"No module named 'pandas'\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 95, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"train.py\\\", line 5, in <module>\\n    import pandas as pd\\n\"\n        }\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-4e0dcb679944>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# specify True for a verbose log\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\myenv\\lib\\site-packages\\azureml\\core\\run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[1;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[0;32m    496\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinal_details\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"error\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfinal_details\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mActivityFailedException\u001b[0m: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"No module named 'pandas'\",\n        \"details\": [],\n        \"debugInfo\": {\n            \"type\": \"ModuleNotFoundError\",\n            \"message\": \"No module named 'pandas'\",\n            \"stackTrace\": \"  File \\\"azureml-setup/context_manager_injector.py\\\", line 95, in execute_with_context\\n    runpy.run_path(sys.argv[0], globals(), run_name=\\\"__main__\\\")\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 263, in run_path\\n    pkg_name=pkg_name, script_name=fname)\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 96, in _run_module_code\\n    mod_name, mod_spec, pkg_name, script_name)\\n  File \\\"/azureml-envs/azureml_088906b35cdd6596b350e033526a1806/lib/python3.6/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"train.py\\\", line 5, in <module>\\n    import pandas as pd\\n\"\n        }\n    }\n}"
     ]
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=False) # specify True for a verbose log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/55_batchai_execution.txt', 'azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'azureml-logs/azureml.log', 'outputs/sklearn_mnist_model.pkl']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn_mnist\tsklearn_mnist:1\t1\n"
     ]
    }
   ],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='sklearn_mnist', model_path='outputs/sklearn_mnist_model.pkl')\n",
    "print(model.name, model.id, model.version, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
